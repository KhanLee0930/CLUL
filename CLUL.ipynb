{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf742fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If U are using SageMaker Prepare for the dataset\n",
    "!pip install awscli\n",
    "!aws s3 cp s3://handata/ref_youtube_audio/ ref_youtube_audio/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install -U openai-whisper\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e7a758-d85d-444c-aede-9f31a6fe37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import whisper\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from categories import ytvos_category_dict\n",
    "import numpy as np\n",
    "from util import read_aws_json,read_aws_wav,read_local_json,read_local_wav\n",
    "import logging\n",
    "from torch import optim\n",
    "from losses import get_loss_func\n",
    "from utils.evaluate import Evaluator\n",
    "from util import infoNCE_loss\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from enum import Enum\n",
    "from sklearn.metrics import f1_score,precision_recall_curve,precision_score,recall_score,accuracy_score,balanced_accuracy_score\n",
    "from collections import Counter\n",
    "from audiomentations import Compose, Gain, AddGaussianNoise, PitchShift,TimeStretch,Shift\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "SageMaker = False\n",
    "Local = True\n",
    "# ROOT = 'C:/Users/Administrator/Desktop/CLUL-main/data/'\n",
    "# SAVEDIR = 'C:/Users/Administrator/Desktop/CLUL-main/run/'\n",
    "ROOT = '/root/CLUL/data/'\n",
    "SAVEDIR = '/root/CLUL/run/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a734c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Audio_Encoder(nn.Module):\n",
    "    def __init__(self, feature_extractor, model, num_class=66,dropout_prob=0.2,pool_num = 100,bias = True):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.encoder = model.encoder\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "          param.requires_grad = False\n",
    "        self.projector = nn.Linear(in_features=768, out_features=256, bias=True)\n",
    "        self.classifier = nn.Linear(256, num_class)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=(pool_num,1), stride=(pool_num,1))\n",
    "        # self.norm_layer = nn.LayerNorm(256, eps=1e-5, bias=True)\n",
    "        self.batchnorm = nn.BatchNorm1d(2048, affine=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(1500//pool_num * 256, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 256)\n",
    "        self.fc3 = nn.Linear(256, num_class)\n",
    "\n",
    "    def forward(self, audios):\n",
    "        input_features = []\n",
    "        for audio in audios:\n",
    "\n",
    "            feature = self.feature_extractor(audio.cpu(),sampling_rate=16000,return_tensors=\"pt\").input_features\n",
    "            input_features.append(feature)\n",
    "\n",
    "        input_features = torch.cat(input_features, dim=0).to(self.device)\n",
    "        hidden_states = self.encoder(input_features)\n",
    "        # hidden_states = self.projector(hidden_states)\n",
    "        # pooled_output = hidden_states.mean(dim=1)\n",
    "        # logits = self.classifier(pooled_output)\n",
    "\n",
    "        x = self.avg_pool(hidden_states)\n",
    "\n",
    "        x = self.projector(x)\n",
    "        # x = self.positionencoding(x)\n",
    "        feature = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.dropout(feature)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        # x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        output_dict = {\n",
    "            'clipwise_output': x,\n",
    "            'feature': feature,\n",
    "            'embedding': hidden_states}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "class ytvos_Dataset(Dataset):\n",
    "    def __init__(self, data_frame: pd.DataFrame, sr=44100, num_class=66):\n",
    "        self.data_frame = data_frame\n",
    "        self.sr = sr\n",
    "        self.num_class = num_class\n",
    "        self.data_root = '/home/user/SED_Adaptation_Classifier-main/data/ref_youtube_audio/audio'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        audio_name = self.data_frame.iloc[index][\"video\"]\n",
    "        audio_id = self.data_frame.iloc[index][\"audio\"]\n",
    "        audio_path = 'ref_youtube_audio/audio' + '/' + audio_name + '/' + audio_id + '.wav'\n",
    "        name = audio_name + self.data_frame.iloc[index][\"exp\"]\n",
    "\n",
    "        \n",
    "        waveform = read_local_wav(ROOT + audio_path)\n",
    "#         waveform = whisper.load_audio(audio_path,sr = 16000)\n",
    "\n",
    "        tag = self.data_frame.iloc[index][\"category\"]\n",
    "        target = ytvos_category_dict[self.data_frame.iloc[index][\"category\"]]\n",
    "        target = np.eye(self.num_class)[target]\n",
    "        data_dict = {'audio_name': name, 'waveform': waveform, 'target': target, 'tag': tag}\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "def get_datalist(cur_iter):\n",
    "        task_id = cur_iter\n",
    "        task_train_metas = []\n",
    "        task_test_metas = []\n",
    "\n",
    "       \n",
    "        metas = read_local_json(ROOT + 'task_split_1/metas.json')['metas']\n",
    "        tasks = read_local_json(ROOT + 'task_split_1/task{}.json'.format(task_id))[str(task_id)]\n",
    "\n",
    "        for category,task_metas_dict in tasks.items():\n",
    "            train_ids = task_metas_dict['train']\n",
    "            test_ids = task_metas_dict['test']\n",
    "            for train_id in train_ids:\n",
    "                task_train_metas.append(metas[train_id])\n",
    "            for test_id in test_ids:\n",
    "                task_test_metas.append(metas[test_id])\n",
    "\n",
    "        return task_train_metas,task_test_metas\n",
    "    \n",
    "def default_collate_fn(batch):\n",
    "    audio_name = [data['audio_name'] for data in batch]\n",
    "    waveform = [torch.from_numpy(data['waveform']) for data in batch]\n",
    "    target = [data['target'] for data in batch]\n",
    "\n",
    "    # waveform = torch.FloatTensor(waveform)\n",
    "    # waveform = pad_sequence(waveform, batch_first=True, padding_value=0)\n",
    "    target = torch.FloatTensor(target)\n",
    "\n",
    "    return {'audio_name': audio_name, 'waveform': waveform, 'target': target}\n",
    "\n",
    "def get_dataloader(data_frame, dataset,split, batch_size, num_workers=0):\n",
    "    assert dataset == \"ref_youtube_audio\"\n",
    "    dataset = ytvos_Dataset(data_frame=data_frame)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                      shuffle=(split == 'train'), drop_last=False,\n",
    "                      num_workers=num_workers, collate_fn=default_collate_fn)\n",
    "\n",
    "def get_train_test_dataloader(batch_size, n_worker, train_list, test_list):\n",
    "    train_loader = get_dataloader(pd.DataFrame(train_list), 'ref_youtube_audio','train', batch_size=batch_size, \n",
    "                                  num_workers=n_worker)\n",
    "    test_loader = get_dataloader(pd.DataFrame(test_list), 'ref_youtube_audio','test', batch_size=batch_size, \n",
    "                                 num_workers=n_worker)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3d6cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLUL:\n",
    "    def __init__(self,batch_size = 32,lr = 1e-3,memory_size = 500,\n",
    "                 forget_size = 100,epoch=5,loss ='focal_loss',\n",
    "                 total_class_num = 65,mode = 'All_Learn_then_Forget',\n",
    "                 patience = 5,n_worker = 0,\n",
    "                 **kwargs):\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"/root/CLUL/whisper\")\n",
    "        whisper_model = whisper.load_model(\"small\")\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = Audio_Encoder(feature_extractor, whisper_model).to(self.device)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.logger = logging.getLogger()\n",
    "        self.forget_list = []\n",
    "        self.memory_list = []\n",
    "        self.memory_size = memory_size\n",
    "        self.forget_size = forget_size\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999))\n",
    "        self.scheduler = ExponentialLR(self.optimizer, gamma=0.9)\n",
    "        self.criterion = get_loss_func(loss)\n",
    "        self.num_pretrain_class = 0\n",
    "        self.evaluator = Evaluator(self.model, self.num_pretrain_class, self.device)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.total_class_num = total_class_num\n",
    "        self.forget_label = total_class_num\n",
    "        self.n_worker = n_worker\n",
    "        self.cltask = {\n",
    "            'task0':[15, 17, 60, 50, 32, 24, 63, 36, 31, 40, 52, 4, 25],\n",
    "            \"task1\":[48, 54, 35, 62, 13, 42, 37, 49, 51, 45, 44, 14, 5],\n",
    "            \"task2\":[46, 18, 57, 28, 11, 30, 61, 27, 22, 2, 29, 0, 19],\n",
    "            \"task3\":[3, 59, 10, 12, 8, 1, 26, 23, 34, 58, 64, 56, 41],\n",
    "            \"task4\":[47, 20, 53, 39, 9, 21, 16, 38, 33, 43, 6, 7, 55]\n",
    "        }\n",
    "        self.ultask = {\n",
    "            \"ul_task0\":[],\n",
    "            \"ul_task1\":[15,17,60],\n",
    "            \"ul_task2\": [48, 54, 35],\n",
    "            \"ul_task3\":[46, 18, 57],\n",
    "            \"ul_task4\" : [3, 59, 10],\n",
    "            \"ul_task5\" : [47, 20, 53]\n",
    "        }\n",
    "           \n",
    "    def evaluate(self,model_path,cur_iter):\n",
    "        self.change_model(model_path)\n",
    "        train_list,test_list = self.get_train_test_datalist(cur_iter)\n",
    "        _, test_loader = get_train_test_dataloader(self.batch_size, self.n_worker, train_list, test_list)\n",
    "        y_true,y_pred = self.evaluator.evaluate(test_loader)\n",
    "        \n",
    "        cl_class_label,ul_class_label = self.get_cl_ul_class_label(cur_iter)\n",
    "        # statistics = self.calculate_metrics(y_true,y_pred,cl_class_label,ul_class_label)\n",
    "        # print(y_true,y_pred,cl_class_label,ul_class_label)\n",
    "        # return statistics\n",
    "        return y_true,y_pred,cl_class_label,ul_class_label\n",
    "\n",
    "    def get_train_test_datalist(self,cur_iter):\n",
    "        train_list,test_list = get_datalist(cur_iter)\n",
    "        return train_list,test_list\n",
    "        \n",
    "    def change_model(self, path):\n",
    "        checkpoint_dict = torch.load(path)\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if name in checkpoint_dict:\n",
    "                    param.data.copy_(checkpoint_dict[name])\n",
    "                \n",
    "    def equal_class_sampling(self, samples, num_class):\n",
    "        class_list = [self.cltask[\"task0\"], self.cltask[\"task1\"],self.cltask[\"task2\"],self.cltask[\"task3\"],self.cltask[\"task4\"]]\n",
    "        cur_class_list = []\n",
    "        for i in range(num_class//13):\n",
    "            cur_class_list += class_list[i]\n",
    "        mem_per_cls = self.memory_size // num_class\n",
    "        sample_df = pd.DataFrame(samples)\n",
    "\n",
    "        # Warning: assuming the classes were ordered following task number.\n",
    "        ret = []\n",
    "        for y in cur_class_list:\n",
    "            cls_df = sample_df[(sample_df[\"category\"].map(ytvos_category_dict)) == y]\n",
    "            ret += cls_df.sample(n=min(mem_per_cls, len(cls_df))).to_dict(\n",
    "                orient=\"records\"\n",
    "            )\n",
    "\n",
    "        num_rest_slots = self.memory_size - len(ret)\n",
    "        if num_rest_slots > 0:\n",
    "            self.logger.warning(\"Fill the unused slots by breaking the equilibrium.\")\n",
    "            ret += (\n",
    "                sample_df[~sample_df.exp.isin(pd.DataFrame(ret).exp)]\n",
    "                .sample(n=num_rest_slots)\n",
    "                .to_dict(orient=\"records\")\n",
    "            )\n",
    "\n",
    "        num_dups = pd.DataFrame(ret).exp.duplicated().sum()\n",
    "        if num_dups > 0:\n",
    "            self.logger.warning(f\"Duplicated samples in memory: {num_dups}\")\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def get_data(self, infer_loader, augment):\n",
    "        Z, Z_, predict_list = [], [], []\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for id, data in enumerate(tqdm(infer_loader)):\n",
    "                wavs = data['waveform']\n",
    "                aug_wavs = []\n",
    "                for wav in wavs:\n",
    "                    aug_wav = augment(wav.unsqueeze(0).unsqueeze(0), sample_rate=1600)\n",
    "                    aug_wavs.append(torch.as_tensor(aug_wav.squeeze(0).squeeze(0), dtype=torch.float32))\n",
    "\n",
    "                output_dict = self.model(data['waveform'])\n",
    "                aug_output_dict = self.model(aug_wavs)\n",
    "\n",
    "                Z.extend(output_dict['feature'].cpu())\n",
    "                Z_.extend(aug_output_dict['feature'].cpu())\n",
    "\n",
    "                clipwise_output = output_dict['clipwise_output']\n",
    "                pres = np.argmax(clipwise_output.detach().cpu(), axis=1)\n",
    "                \n",
    "\n",
    "                for pre in pres: predict_list.append(pre.item())\n",
    "\n",
    "            class_label_dic = self.save_indexes(predict_list)\n",
    "        return Z, Z_, class_label_dic, predict_list\n",
    "    \n",
    "    def save_indexes(self,arr):\n",
    "        index_dict = {}\n",
    "        for idx, num in enumerate(arr):\n",
    "            if num in index_dict:\n",
    "                index_dict[num].append(idx)\n",
    "            else:\n",
    "                  index_dict[num] = [idx]\n",
    "        return index_dict\n",
    "\n",
    "    def Boundary_Expansion_Forget_Label_Set(self,y_true,ul_class_label):\n",
    "        index_row = torch.argmax(y_true,dim=1)\n",
    "        for r ,c in enumerate(index_row):\n",
    "            if int(c) in ul_class_label:\n",
    "                with torch.no_grad():\n",
    "                    y_true[r][c] = torch.tensor(0.0, dtype=torch.float32)\n",
    "                    y_true[r][-1] = torch.tensor(1.0, dtype=torch.float32)\n",
    "        return y_true\n",
    "    \n",
    "    def Ramdom_Label_Forget_Label_Set(self,y_true,cl_class_label,ul_class_label):\n",
    "        index_row = torch.argmax(y_true,dim=1)\n",
    "        for r ,c in enumerate(index_row):\n",
    "            if int(c) in ul_class_label:\n",
    "                with torch.no_grad():\n",
    "                    idx = random.choice(list(set(cl_class_label)-set(ul_class_label)))\n",
    "                    y_true[r][c] = torch.tensor(0.0, dtype=torch.float32)\n",
    "                    y_true[r][idx] = torch.tensor(1.0, dtype=torch.float32)\n",
    "        return y_true\n",
    "    def class_infoNCE(self, Z, Z_, class_label_dic, predict_list, temperature):\n",
    "        ## You can change the method to calculate NCEs\n",
    "        NCEs = []\n",
    "        # print('This is cclass_label_dic',class_label_dic)\n",
    "        for id in range(len(predict_list)):\n",
    "            label = predict_list[id]\n",
    "            same_label_list = class_label_dic[label]\n",
    "            class_z = [Z[i] for i in same_label_list if i != id]\n",
    "            class_z_ = [Z_[i] for i in same_label_list]\n",
    "\n",
    "            positive_pair = class_z + class_z_\n",
    "\n",
    "            positive_similarities = F.cosine_similarity(Z[id].unsqueeze(0), torch.stack(positive_pair)) / 2 + 0.5\n",
    "            # print('This is postitive pair info',Z[id].unsqueeze(0).shape,torch.stack(positive_pair).shape,positive_similarities.shape)\n",
    "            positive_value = torch.exp(positive_similarities / temperature).sum() / len(positive_pair)\n",
    "            # print(positive_similarities,positive_value)\n",
    "            neg_labels = [i for i in list(class_label_dic.keys()) if i != label]\n",
    "\n",
    "            negative_values = 0\n",
    "            for neg_label in neg_labels:\n",
    "                neg_label_list = class_label_dic[neg_label]\n",
    "                neg_z = [Z[i] for i in neg_label_list]\n",
    "                neg_z_ = [Z_[i] for i in neg_label_list]\n",
    "                negative_pair = neg_z + neg_z_\n",
    "                negative_similarities = F.cosine_similarity(Z[id].unsqueeze(0), torch.stack(negative_pair)) / 2 + 0.5\n",
    "                # print('This is negative pair info',Z[id].unsqueeze(0).shape,torch.stack(negative_pair).shape,negative_similarities.shape,len(negative_pair))\n",
    "                negative_value = torch.exp(negative_similarities / temperature).sum() / len(negative_pair)\n",
    "                # print(negative_similarities,negative_value)\n",
    "                negative_values += negative_value\n",
    "\n",
    "            NCE = -torch.log(positive_value / (positive_value + negative_values))\n",
    "            # print('positive_value',positive_value,'negative values', negative_values,'this is single nce',NCE)\n",
    "            NCEs.append(NCE)\n",
    "        print(torch.stack(NCEs).shape)\n",
    "        return torch.stack(NCEs)\n",
    "    \n",
    "    def single_mutual_info_sampling(self,cur_iter, train_list, cl_class_label, ul_class_label):\n",
    "        ret_mem = []\n",
    "        val_class_label = list(set(cl_class_label) - set(ul_class_label))\n",
    "        train_df = pd.DataFrame(train_list)\n",
    "        train_df['category_id'] = train_df['category'].map(ytvos_category_dict)\n",
    "\n",
    "        train_df = train_df[train_df['category_id']. isin( val_class_label)]\n",
    "        assert len(Counter(train_df['category'])) == len(val_class_label)\n",
    "\n",
    "        inf_loader = get_dataloader(train_df, 'ref_youtube_audio', 'test', self.batch_size, self.n_worker)\n",
    "        temperature = 0.05\n",
    "\n",
    "\n",
    "        augment = Compose([\n",
    "            # Gain(min_gain_in_db=-12.0, max_gain_in_db=12.0),\n",
    "            # AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.001),\n",
    "            PitchShift(min_semitones=-0.5, max_semitones=0.5, p=0.5),\n",
    "            # AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015),\n",
    "            # TimeShift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "            # Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n",
    "            # TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "        ])\n",
    "\n",
    "        #Calculate current infoNCE\n",
    "        Z, Z_, class_label_dic, predict_list = self.get_data(inf_loader, augment)\n",
    "        assert (len(Z) == len(Z_) == len(predict_list))\n",
    "        cur_NCEs = self.class_infoNCE(Z, Z_, class_label_dic, predict_list, temperature)\n",
    "\n",
    "\n",
    "        #Calculate previous infoNCE\n",
    "\n",
    "        self.change_model(SAVEDIR + '{}/task{}best_epoch2.pt'.format(self.mode,cur_iter-1))\n",
    "        pre_Z, pre_Z_, pre_class_label_dic, pre_predict_list = self.get_data(inf_loader, augment)\n",
    "        assert (len(Z) == len(Z_) == len(predict_list))\n",
    "        pre_NCEs = self.class_infoNCE(pre_Z, pre_Z_, pre_class_label_dic, pre_predict_list, temperature)\n",
    "        self.change_model(SAVEDIR + '{}/task{}best_epoch2.pt'.format(self.mode,cur_iter))\n",
    "\n",
    "        \n",
    "\n",
    "        NCEs = pre_NCEs - cur_NCEs\n",
    "        train_df['NCE'] = NCEs\n",
    "\n",
    "        mem_per_cls = self.memory_size // len(val_class_label)\n",
    "\n",
    "        for i in val_class_label:\n",
    "            cls_df = train_df[(train_df[\"category\"].map(ytvos_category_dict)) == i]\n",
    "            if len(cls_df) <= mem_per_cls:\n",
    "                ret_mem += cls_df.to_dict(orient=\"records\")\n",
    "            else:\n",
    "                jump_idx = len(cls_df) // mem_per_cls\n",
    "                uncertain_samples = cls_df.sort_values(by=\"NCE\")[::jump_idx]\n",
    "                ret_mem += uncertain_samples[:mem_per_cls].to_dict(orient=\"records\")\n",
    "\n",
    "        num_rest_slots = self.memory_size - len(ret_mem)\n",
    "        if num_rest_slots > 0:\n",
    "            self.logger.warning(\"Fill the unused slots by breaking the equilibrium.\")\n",
    "            ret_mem += (\n",
    "                train_df[~train_df.exp.isin(pd.DataFrame(ret_mem).exp)]\n",
    "                .sample(n=num_rest_slots)\n",
    "                .to_dict(orient=\"records\")\n",
    "            )\n",
    "\n",
    "        num_dups = pd.DataFrame(ret_mem).exp.duplicated().sum()\n",
    "        if num_dups > 0:\n",
    "            self.logger.warning(f\"Duplicated samples in memory: {num_dups}\")\n",
    "\n",
    "\n",
    "        class_count = Counter(pd.DataFrame(ret_mem)['category'])\n",
    "        print('After Unpdate Statistics',class_count)\n",
    "        \n",
    "        return ret_mem\n",
    "      \n",
    "    def double_mutual_info_sampling(self, candidates, cur, num_class):\n",
    "        from audiomentations import Compose, Gain, AddGaussianNoise, PitchShift,TimeStretch,Shift\n",
    "        from collections import Counter\n",
    "        \n",
    "        ulclass_list =   [None,self.ultask[\"task1\"],self.ultask[\"task2\"],self.ultask[\"task3\"],self.ultask[\"task4\"]]\n",
    "        class_list = [self.cltask[\"task0\"], self.cltask[\"task1\"],self.cltask[\"task2\"],self.cltask[\"task3\"],self.cltask[\"task4\"]]\n",
    "        cl_class_list = []\n",
    "        ul_class_list = []\n",
    "        for i in range(num_class // 13):\n",
    "            cur_class_list |= set(class_list[i])\n",
    "            cur_class_list -= set(ulclass_list[i])\n",
    "        cur_class_list.add(self.total_class_num-1)\n",
    "        # Unlearning Part:class deleted will not be added into the memory bank\n",
    "\n",
    "        infer_df = pd.DataFrame(candidates)\n",
    "\n",
    "        class_count = Counter(infer_df['category'])\n",
    "        print('Before Unpdate Statistics')\n",
    "        for name, number in class_count.items():\n",
    "            print(name, number)\n",
    "        # mem_per_cls = self.memory_size // num_class  # kc: the number of the samples of each class\n",
    "\n",
    "        batch_size = 8\n",
    "        temperature = 0.05\n",
    "        ret = []\n",
    "        infer_loader = get_dataloader(infer_df, 'ref_youtube_audio', split='test', batch_size=batch_size, num_class=num_class,\n",
    "                                      num_workers=8)\n",
    "        augment = Compose([\n",
    "            # Gain(min_gain_in_db=-12.0, max_gain_in_db=12.0),\n",
    "            # AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.001),\n",
    "            PitchShift(min_semitones=-0.5, max_semitones=0.5, p=0.5),\n",
    "            # AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015),\n",
    "            # TimeShift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "            # Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n",
    "            # TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "        ])\n",
    "\n",
    "        Z, Z_, class_label_dic, predict_list = self.get_data(infer_loader, augment)\n",
    "        assert (len(Z) == len(Z_) == len(predict_list))\n",
    "\n",
    "        cur_NCEs = self.class_infoNCE(Z, Z_, class_label_dic, predict_list, temperature)\n",
    "\n",
    "        path = '/home/user/SED_Adaptation_Classifier-main/workspace/ref_youtube/MIO/iter{}epoch.pt'.format(cur - 1)\n",
    "        self.change_model(path)\n",
    "\n",
    "        pre_Z, pre_Z_, pre_class_label_dic, pre_predict_list = self.get_data(infer_loader, augment)\n",
    "        assert (len(Z) == len(Z_) == len(predict_list))\n",
    "\n",
    "        pre_NCEs = self.class_infoNCE(pre_Z, pre_Z_, pre_class_label_dic, pre_predict_list, temperature)\n",
    "\n",
    "        path = '/home/user/SED_Adaptation_Classifier-main/workspace/ref_youtube/MIO/iter{}epoch.pt'.format(cur)\n",
    "        self.change_model(path)\n",
    "\n",
    "        # print(len(Z),len(Z_),len(predict_list),len(candidates))\n",
    "\n",
    "        NCEs = pre_NCEs - cur_NCEs\n",
    "        for candidate,NCE in zip(candidates,NCEs):candidate['NCE'] = NCE\n",
    "\n",
    "        sample_df = pd.DataFrame(candidates)\n",
    "         # kc: the number of the samples of each class in memory bank\n",
    "        mem_per_cls = self.memory_size // len(cl_class_list)\n",
    "        \n",
    "        for_per_cls = self.forget_size// len(ul_class_list)\n",
    "        \n",
    "\n",
    "\n",
    "        for i in cur_class_list:\n",
    "            cls_df = sample_df[(sample_df[\"category\"].map(ytvos_category_dict)) == i]\n",
    "            if len(cls_df) <= mem_per_cls:\n",
    "                ret += cls_df.to_dict(orient=\"records\")\n",
    "            else:\n",
    "                jump_idx = len(cls_df) // mem_per_cls\n",
    "                uncertain_samples = cls_df.sort_values(by=\"NCE\")[::jump_idx]\n",
    "                ret += uncertain_samples[:mem_per_cls].to_dict(orient=\"records\")\n",
    "\n",
    "        num_rest_slots = self.memory_size - len(ret)\n",
    "        if num_rest_slots > 0:\n",
    "            logger.warning(\"Fill the unused slots by breaking the equilibrium.\")\n",
    "            ret += (\n",
    "                sample_df[~sample_df.exp.isin(pd.DataFrame(ret).exp)]\n",
    "                .sample(n=num_rest_slots)\n",
    "                .to_dict(orient=\"records\")\n",
    "            )\n",
    "\n",
    "        num_dups = pd.DataFrame(ret).exp.duplicated().sum()\n",
    "        if num_dups > 0:\n",
    "            logger.warning(f\"Duplicated samples in memory: {num_dups}\")\n",
    "\n",
    "\n",
    "        # top_indices = np.argpartition(NCEs.cpu().numpy(), -2000)[-2000:]\n",
    "        #\n",
    "        # for index in top_indices:\n",
    "        #     ret.append(candidates[index])\n",
    "\n",
    "        class_count = Counter(pd.DataFrame(ret)['category'])\n",
    "        print('After Unpdate Statistics')\n",
    "        for name, number in class_count.items():\n",
    "            print(name, number)\n",
    "\n",
    "        return ret\n",
    "    \n",
    "#     def train_with_datalist(self,train_list,test_list):\n",
    "        \n",
    "        train_loader, test_loader = get_train_test_dataloader(self.batch_size, self.n_worker, train_list, test_list)\n",
    "        self.logger.info(f\"In-memory samples: {len(self.memory_list)}\")\n",
    "        self.logger.info(f\"Train samples: {len(train_list)}\")\n",
    "        self.logger.info(f\"Test samples: {len(test_list)}\")\n",
    "        # logger.info(f\"Model: {self.model}\")\n",
    "        self.logger.info(f\"Optimizer: {self.optimizer}\")\n",
    "        acc_list = []\n",
    "        best = {'acc': 0, 'epoch': 0,'f1_score':0}\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            mean_loss = 0\n",
    "            for idx,batch_data_dict in enumerate(tqdm(train_loader)):\n",
    "                batch_data_dict['waveform'] = batch_data_dict['waveform']\n",
    "                batch_data_dict['target'] = batch_data_dict['target'].to(self.device)\n",
    "\n",
    "                # Forward\n",
    "                self.model.train()\n",
    "\n",
    "                batch_output_dict = self.model(batch_data_dict['waveform'])\n",
    "                \"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\n",
    "                batch_target_dict = {'target': batch_data_dict['target']}\n",
    "                \"\"\"{'target': (batch_size, classes_num)}\"\"\"\n",
    "                # Loss\n",
    "                \n",
    "                loss = self.criterion(batch_output_dict, batch_target_dict)\n",
    "                self.logger.info(f'Batch Training Initial Loss: {loss}')\n",
    "                if idx % 10 == 0:\n",
    "                    print(f'Epoch:{epoch},Batch {idx} Loss: {loss}')\n",
    "                # Backwards\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                loss = loss.item()\n",
    "\n",
    "                mean_loss += loss\n",
    "            epoch_loss = mean_loss / len(train_loader)\n",
    "            self.logger.info(f'Epoch {epoch} | Training Loss: {epoch_loss}')\n",
    "            print(f'Epoch {epoch} | Training Loss: {epoch_loss}')\n",
    "            # Evaluate\n",
    "            test_statistics = self.evaluator.evaluate(test_loader)\n",
    "            ave_f1_score = np.mean(test_statistics['f1_score'])\n",
    "            ave_acc = np.mean(test_statistics['accuracy'])\n",
    "            acc_list.append(ave_acc)\n",
    "            self.logger.info(f\"Epoch {epoch} | Evaluation Accuracy: {ave_acc}|Evaluation f1_score: {ave_f1_score}\")\n",
    "            self.logger.info(f'Current Accuracy: {ave_acc} in epoch {epoch}.|Current f1_score: {ave_f1_score} in epoch {epoch}.')\n",
    "            print(f\"Task {cur_iter} | Epoch {epoch} | Evaluation Accuracy: {ave_acc}|Evaluation f1_score: {ave_f1_score}|Evaluation precision {test_statistics['precision']}\")\n",
    "            \n",
    "\n",
    "            if ave_f1_score > best['f1_score']:\n",
    "                best['acc'] = ave_acc\n",
    "                best['f1_score'] = ave_f1_score\n",
    "                best['epoch'] = epoch\n",
    "                self.logger.info(f'Best Accuracy: {ave_acc} in epoch {epoch}.|Best f1_score: {ave_f1_score} in epoch {epoch}.')\n",
    "                selected_state_dict = {}\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if 'projector' in name or 'classifier' in name or 'fc' in name and ('encoder' not in name):\n",
    "                        selected_state_dict[name] = param\n",
    "                torch.save(selected_state_dict,SAVEDIR + '{}/task{}best_epoch{}.pt'.format(self.mode,cur_iter,epoch))\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                self.logger.info(f'EarlyStopping counter: {self.counter} out of {self.patience}.')\n",
    "                if self.counter >= self.patience:\n",
    "                    break\n",
    "        print(f\"Task {cur_iter} | Best Epoch {best['epoch']} | Best Evaluation Accuracy: {best['acc']}|Evaluation f1_score: {best['f1_score']}\")\n",
    "        return \n",
    "    \n",
    "    def calculate_metrics(self,y_true,y_pred,cl_class_label,ul_class_label):\n",
    "        statistics = {'cl_weighted_accuracy':0,'ul_weighted_accuracy':0,'cl_accuracy':0,'ul_accuracy':0}\n",
    "        cl_y_true,cl_y_pred = [],[]\n",
    "        ul_y_true,ul_y_pred = [],[]\n",
    "        for y_t,y_d in zip(y_true,y_pred):\n",
    "            if y_t in ul_class_label:\n",
    "                ul_y_true.append(y_t)\n",
    "                ul_y_pred.append(y_d)\n",
    "            else:\n",
    "                cl_y_true.append(y_t)\n",
    "                cl_y_pred.append(y_d)\n",
    "        cl_weighted_accuracy = balanced_accuracy_score(cl_y_true,cl_y_pred)\n",
    "        ul_weighted_accuracy = balanced_accuracy_score(ul_y_true,ul_y_pred)\n",
    "\n",
    "        cl_accuracy = accuracy_score(cl_y_true,cl_y_pred)\n",
    "        ul_accuracy = accuracy_score(ul_y_true,ul_y_pred)\n",
    "\n",
    "        statistics['ul_accuracy'] = ul_accuracy\n",
    "        statistics['cl_accuracy'] = cl_accuracy\n",
    "\n",
    "        statistics['cl_weighted_accuracy'] = cl_weighted_accuracy\n",
    "        statistics['ul_weighted_accuracy'] = ul_weighted_accuracy\n",
    "        # print(cl_y_true,cl_y_pred)\n",
    "        # print(ul_y_true,ul_y_pred)\n",
    "        return statistics\n",
    "\n",
    "    def get_cl_ul_class_label(self,cur_iter):\n",
    "        cl_class_label = self.cltask[f'task{cur_iter}']\n",
    "        ul_class_label = self.ultask[f'ul_task{cur_iter}']\n",
    "        return cl_class_label,ul_class_label\n",
    "    \n",
    "#     def train_with_forget_without_forget_bank(self, cur_iter,mode = 'all_learn_forget'):\n",
    "        \n",
    "#         memory_bank = self.memory_list\n",
    "#         test_list = []\n",
    "#         for i in range(cur_iter + 1):\n",
    "#             train_list_,test_data_list_ = get_datalist(i)\n",
    "#             test_list += test_data_list_\n",
    "        \n",
    "#         train_list,_ = get_datalist(cur_iter)\n",
    "#         train_list += memory_bank\n",
    "\n",
    "#         train_loader,test_loader = get_train_test_dataloader(self.batch_size, self.n_worker, train_list, test_list)\n",
    "#         cl_class_label,ul_class_label = [],[]\n",
    "\n",
    "#         best = {'cl_weighted_accuracy':0,'cl_accuracy':0,'ul_weighted_accuracy':0,'ul_accuracy':0,'epoch':0}\n",
    "#         for i in range(cur_iter + 1):\n",
    "#             cl_class_label += self.cltask[f'task{i}']\n",
    "#             ul_class_label += self.ultask[f'ul_task{i}']\n",
    "#         print('train loader length',len(train_loader),'test loader length',len(test_loader),'cl class label',cl_class_label,'ul class label',ul_class_label)\n",
    "#         for epoch in range(self.epoch):\n",
    "#             mean_loss = 0\n",
    "#             for idx,batch_data_dict in enumerate(tqdm(train_loader)):\n",
    "#                 batch_data_dict['waveform'] = batch_data_dict['waveform']\n",
    "#                 # print(batch_data_dict['target'],ul_class_label)\n",
    "#                 batch_data_dict['target'] = self.forget_label_set(batch_data_dict['target'],ul_class_label)\n",
    "#                 batch_data_dict['target'] = batch_data_dict['target'].to(self.device)\n",
    "\n",
    "#                 # Forward\n",
    "#                 self.model.train()\n",
    "\n",
    "#                 batch_output_dict = self.model(batch_data_dict['waveform'])\n",
    "#                 \"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\n",
    "#                 batch_target_dict = {'target': batch_data_dict['target']}\n",
    "#                 \"\"\"{'target': (batch_size, classes_num)}\"\"\"\n",
    "#                 # Loss\n",
    "                \n",
    "#                 loss = self.criterion(batch_output_dict, batch_target_dict)\n",
    "#                 self.logger.info(f'Batch Training Initial Loss: {loss}')\n",
    "#                 if idx % 10 == 0:\n",
    "#                     print(f'Epoch:{epoch},Batch {idx} Loss: {loss}')\n",
    "#                 # Backwards\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 self.optimizer.zero_grad()\n",
    "\n",
    "#                 loss = loss.item()\n",
    "\n",
    "#                 mean_loss += loss\n",
    "#             epoch_loss = mean_loss / len(train_loader)\n",
    "            \n",
    "#             print(f'Epoch {epoch} | Training Loss: {epoch_loss}')\n",
    "#             # Evaluate\n",
    "#             y_true,y_pred = self.evaluator.evaluate(test_loader)\n",
    "\n",
    "#             statistics = self.calculate_metrics(y_true,y_pred,cl_class_label,ul_class_label)\n",
    "\n",
    "#             print(f\"Task {cur_iter} |  Epoch {epoch} | statistics {statistics}\")\n",
    "#             if  statistics['cl_weighted_accuracy'] > best['cl_weighted_accuracy']:\n",
    "#                 best['cl_weighted_accuracy'] = statistics['cl_weighted_accuracy']\n",
    "#                 best['cl_accuracy'] = statistics['cl_accuracy']\n",
    "#                 best['epoch'] = epoch\n",
    "#                 # self.logger.info(f'Best Accuracy: {accuracy} in epoch {epoch}.|Best weighted_accuracy: {weighted_accuracy} in epoch {epoch}.')\n",
    "#                 selected_state_dict = {}\n",
    "#                 for name, param in self.model.named_parameters():\n",
    "#                     if 'projector' in name or 'classifier' in name or 'fc' in name and ('encoder' not in name):\n",
    "#                         selected_state_dict[name] = param\n",
    "#                 torch.save(selected_state_dict,SAVEDIR + '{}/task{}best_epoch{}.pt'.format(self.mode,cur_iter,epoch))\n",
    "#                 self.counter = 0\n",
    "#             else:\n",
    "#                 self.counter += 1\n",
    "#                 self.logger.info(f'EarlyStopping counter: {self.counter} out of {self.patience}.')\n",
    "#                 if self.counter >= self.patience:\n",
    "#                     break\n",
    "#         print(f\"Task {cur_iter} | Best Epoch {best['epoch']} | Best Accuracy: {best['cl_accuracy']}|Best weighted_accuracy: {best['cl_weighted_accuracy']}\")\n",
    "#         return train_list,test_list,cl_class_label,ul_class_label\n",
    "    \n",
    "    def train(self,mode,train_list,test_list,cl_class_label,ul_class_label,cur_iter):\n",
    "        \n",
    "        # method = 'Boundary_Expansion'\n",
    "        method = 'Random_Label'\n",
    "        train_loader,test_loader = get_train_test_dataloader(self.batch_size, self.n_worker, train_list, test_list)\n",
    "        best = {'epoch':0,'cl_weighted_accuracy':0,'ul_weighted_accuracy':0,'cl_accuracy':0,'ul_accuracy':0}\n",
    "        print('train loader length',len(train_loader),'test loader length',len(test_loader),'cl class label',cl_class_label,'ul class label',ul_class_label)\n",
    "    \n",
    "        for epoch in range(self.epoch):\n",
    "            mean_loss = 0\n",
    "            for idx,batch_data_dict in enumerate(tqdm(train_loader)):\n",
    "                batch_data_dict['waveform'] = batch_data_dict['waveform']\n",
    "                # print(batch_data_dict['target'],ul_class_label)\n",
    "                if method == 'Boundary_Expansion':\n",
    "                    batch_data_dict['target'] = self.Boundary_Expansion_Forget_Label_Set(batch_data_dict['target'],ul_class_label)\n",
    "                if method == 'Random_Label':\n",
    "                    batch_data_dict['target'] = self.Ramdom_Label_Forget_Label_Set(batch_data_dict['target'],cl_class_label,ul_class_label)\n",
    "                    \n",
    "                batch_data_dict['target'] = batch_data_dict['target'].to(self.device)\n",
    "                \n",
    "                # print(\"=============This is input data==============\")\n",
    "                # print(batch_data_dict['target'])\n",
    "                # Forward\n",
    "                self.model.train()\n",
    "\n",
    "                batch_output_dict = self.model(batch_data_dict['waveform'])\n",
    "                \"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\n",
    "                batch_target_dict = {'target': batch_data_dict['target']}\n",
    "                \"\"\"{'target': (batch_size, classes_num)}\"\"\"\n",
    "                # Loss\n",
    "                \n",
    "                loss = self.criterion(batch_output_dict, batch_target_dict)\n",
    "                self.logger.info(f'Batch Training Initial Loss: {loss}')\n",
    "                if idx % 10 == 0:\n",
    "                    print(f'Epoch:{epoch},Batch {idx} Loss: {loss}')\n",
    "                # Backwards\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                # self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                loss = loss.item()\n",
    "\n",
    "                mean_loss += loss\n",
    "            epoch_loss = mean_loss / len(train_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch} | Training Loss: {epoch_loss}')\n",
    "            # Evaluate\n",
    "            y_true,y_pred = self.evaluator.evaluate(test_loader)\n",
    "            \n",
    "            statistics = self.calculate_metrics(y_true,y_pred,cl_class_label,ul_class_label)\n",
    "\n",
    "            print(f\"Task {cur_iter} |  Current Epoch {epoch} | statistics {statistics}\")\n",
    "            if  statistics['cl_weighted_accuracy'] > best['cl_weighted_accuracy']:\n",
    "                best['cl_weighted_accuracy'] = statistics['cl_weighted_accuracy']\n",
    "                best['cl_accuracy'] = statistics['cl_accuracy']\n",
    "                best['epoch'] = epoch\n",
    "                selected_state_dict = {}\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if 'projector' in name or 'classifier' in name or 'fc' in name and ('encoder' not in name):\n",
    "                        selected_state_dict[name] = param\n",
    "                torch.save(selected_state_dict,SAVEDIR + '{}/task{}best_epoch.pt'.format(self.mode,cur_iter))\n",
    "                print('Save Model Successfully',SAVEDIR + '{}/task{}best_epoch.pt'.format(self.mode,cur_iter))\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                self.logger.info(f'EarlyStopping counter: {self.counter} out of {self.patience}.')\n",
    "                if self.counter >= self.patience:\n",
    "                    break\n",
    "            self.scheduler.step()\n",
    "        print(f\"Task {cur_iter} | Best epoch  Best Statistics {best}\")\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def evaluate(self,model_path,test_datalist,cl_class_label,ul_class_label):\n",
    "        self.change_model(model_path)\n",
    "        test_loader = get_dataloader(pd.DataFrame(test_list), 'ref_youtube_audio','test', batch_size=self.batch_size, num_workers=self.n_worker)\n",
    "        y_true,y_pred = self.evaluator.evaluate(test_loader)\n",
    "        statistics = self.calculate_metrics(y_true,y_pred,cl_class_label,ul_class_label)\n",
    "        print(statistics)\n",
    "        return statistics\n",
    "    \n",
    "    def evaluate_self(self,test_datalist,cl_class_label,ul_class_label):\n",
    "        test_loader = get_dataloader(pd.DataFrame(test_list), 'ref_youtube_audio','test', batch_size=self.batch_size, num_workers=self.n_worker)\n",
    "        y_true,y_pred = self.evaluator.evaluate(test_loader)\n",
    "        statistics = self.calculate_metrics(y_true,y_pred,cl_class_label,ul_class_label)\n",
    "        print(statistics)\n",
    "        return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a97a20b-b8d5-4d48-8f08-293b721faa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Training and each stage forget\n",
    "#Each Stage train the all Data Need to Remember and all Data Need to be Forget(Memory bank full,Forget bank full)\n",
    "def Step_Learn_Froeget_Full_memory_Full_forget():\n",
    "    clul = CLUL(epoch = 10)\n",
    "    for idx in range(5):\n",
    "        clul.train_with_forget_without_forget_bank(idx)\n",
    "        train_list,_ = clul.get_train_test_datalist(0)\n",
    "        clul.memory += train_list\n",
    "\n",
    "#At Last Stage learn and forget together\n",
    "def All_Learn_Forget_No_memory_No_forget():\n",
    "    clul = CLUL(epoch = 10)\n",
    "    train_list,test_list = [],[]\n",
    "    cl_class_label,ul_class_label = [],[]\n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        train_list += train_list_\n",
    "        test_list += test_list_\n",
    "        cl_class_label += cl_class_label_\n",
    "        ul_class_label += ul_class_label_\n",
    "        \n",
    "    print('train list length',len(train_list),'test list length',len(test_list),\n",
    "      'cl class label length',len(cl_class_label),'ul class label length',len(ul_class_label))\n",
    "    clul.train('All_Learn_Forget_No_memory_No_forget',train_list,test_list,cl_class_label,ul_class_label,4)\n",
    "\n",
    "def All_Learn_then_Forget():\n",
    "    clul = CLUL(epoch = 5,mode='All_Learn_then_Forget')\n",
    "    train_list,test_list = [],[]\n",
    "    cl_class_label,ul_class_label = [],[]\n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        train_list += train_list_\n",
    "        test_list += test_list_\n",
    "        cl_class_label += cl_class_label_\n",
    "        \n",
    "    print('train list length',len(train_list),'test list length',len(test_list),\n",
    "      'cl class label length',len(cl_class_label),'ul class label length',len(ul_class_label))\n",
    "    clul.train('All_Learn_then_Forget',train_list,test_list,cl_class_label,ul_class_label,4)\n",
    "\n",
    "    \n",
    "    train_list,test_list = [],[]\n",
    "    cl_class_label,ul_class_label = [],[]\n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        train_list += train_list_\n",
    "        test_list += test_list_\n",
    "        ul_class_label += ul_class_label\n",
    "    \n",
    "    train_list = pd.DataFrame(train_list)[(pd.DataFrame(train_list)[\"category\"].map(ytvos_category_dict)).isin(ul_class_label)].to_dict(orient=\"records\")\n",
    "    print('train list length',len(train_list),'test list length',len(test_list),\n",
    "      'cl class label length',len(cl_class_label),'ul class label length',len(ul_class_label))\n",
    "    clul.train('All_Learn_then_Forget',train_list,test_list,cl_class_label,ul_class_label,4)\n",
    "      \n",
    "def Naive_CL():\n",
    "    clul = CLUL(epoch=5,mode= 'Naive_CL')\n",
    "    test_list = []\n",
    "    cl_class_label = []\n",
    "\n",
    "    \n",
    "    for idx in range(5):\n",
    "        train_list,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        cl_class_label += cl_class_label_\n",
    "        test_list += test_list_\n",
    "        cl_class_label = list(set(cl_class_label))\n",
    "        \n",
    "        clul.train('Naive_CL',train_list,test_list,cl_class_label,[],idx)\n",
    "\n",
    "def Full_Memory_CL():\n",
    "    clul = CLUL(epoch=5,mode= 'Full_Memory_CL')\n",
    "    test_list = []\n",
    "    cl_class_label = []\n",
    "\n",
    "    \n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        cl_class_label += cl_class_label_\n",
    "        test_list += test_list_\n",
    "        train_list += train_list_\n",
    "        cl_class_label = list(set(cl_class_label))\n",
    "        \n",
    "        clul.train('Full_Memory_CL',train_list,test_list,cl_class_label,[],idx)\n",
    "    \n",
    "\n",
    "def Learn_All():\n",
    "    clul = CLUL(epoch = 10,mode='Learn_All')\n",
    "    \n",
    "     \n",
    "    train_list,test_list = [],[]\n",
    "    cl_class_label,ul_class_label = [],[]\n",
    "    \n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        train_list += train_list_\n",
    "        test_list += test_list_\n",
    "        cl_class_label += cl_class_label_\n",
    "        ul_class_label += ul_class_label_\n",
    "        \n",
    "\n",
    "    print('Learn train list length',len(train_list),'test list length',len(test_list),\n",
    "      'cl class label length',len(cl_class_label),'ul class label length',len([]))\n",
    "    \n",
    "    clul.train('Learn_All',train_list,test_list,cl_class_label,[],4)\n",
    "\n",
    "#Fine tune the data on the D_r\n",
    "def Learn_CL():\n",
    "    clul = CLUL(epoch = 3,mode='Learn_CL')\n",
    "    clul.change_model('/root/CLUL/run/Learn_All/task4best_epoch.pt')\n",
    "     \n",
    "    train_list,test_list = [],[]\n",
    "    cl_class_label,ul_class_label = [],[]\n",
    "    \n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        train_list += train_list_\n",
    "        test_list += test_list_\n",
    "        cl_class_label += cl_class_label_\n",
    "        ul_class_label += ul_class_label_\n",
    "        \n",
    "    cl_class_label = list(set(cl_class_label)-set(ul_class_label))\n",
    "    train_list = pd.DataFrame(train_list)[(pd.DataFrame(train_list)[\"category\"].map(ytvos_category_dict)).isin(cl_class_label)].to_dict(orient=\"records\")\n",
    "    print('Learn train list length',len(train_list),'test list length',len(test_list),\n",
    "      'cl class label length',len(cl_class_label),'ul class label length',len(ul_class_label))\n",
    "    \n",
    "    clul.train('Learn_CL',train_list,test_list,cl_class_label,ul_class_label,4)\n",
    "\n",
    "\n",
    "def Forget_UL_Boundary_Expansion():\n",
    "    clul = CLUL(epoch = 3,mode='Forget_UL_Boundary_Expansion')\n",
    "    clul.change_model('/root/CLUL/run/Learn_All/task4best_epoch.pt')\n",
    "    \n",
    "    # [15, 17, 60, 48, 54, 35, 46, 18, 57, 3, 59, 10]\n",
    "    train_list,test_list = [],[]\n",
    "    cl_class_label,ul_class_label = [],[15]\n",
    "    \n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        train_list += train_list_\n",
    "        test_list += test_list_\n",
    "        cl_class_label += cl_class_label_\n",
    "    train_list = pd.DataFrame(train_list)[(pd.DataFrame(train_list)[\"category\"].map(ytvos_category_dict)).isin(ul_class_label)].to_dict(orient=\"records\")\n",
    "    print('forget train list length',len(train_list),'test list length',len(test_list),\n",
    "      'cl class label length',len(cl_class_label),'ul class label length',len(ul_class_label))\n",
    "    \n",
    "    clul.train('Forget_UL_Boundary_Expansion',train_list,test_list,cl_class_label,ul_class_label,4)\n",
    "\n",
    "    \n",
    "def Forget_UL_Random_label():\n",
    "    clul = CLUL(epoch = 3,mode='Forget_UL_Random_label')\n",
    "    clul.change_model('/root/CLUL/run/Learn_All/task4best_epoch.pt')\n",
    "     \n",
    "    train_list,test_list = [],[]\n",
    "    cl_class_label,ul_class_label = [],[]\n",
    "    \n",
    "    for idx in range(5):\n",
    "        train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "        cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "        train_list += train_list_\n",
    "        test_list += test_list_\n",
    "        cl_class_label += cl_class_label_\n",
    "        ul_class_label += ul_class_label_\n",
    "        \n",
    "    ul_class_label = list(set(cl_class_label) - set(ul_class_label))\n",
    "    train_list = pd.DataFrame(train_list)[(pd.DataFrame(train_list)[\"category\"].map(ytvos_category_dict)).isin(ul_class_label)].to_dict(orient=\"records\")\n",
    "    print('forget train list length',len(train_list),'test list length',len(test_list),\n",
    "      'cl class label length',len(cl_class_label),'ul class label length',len(ul_class_label))\n",
    "    \n",
    "    clul.train('All_Learn_then_Forget',train_list,test_list,cl_class_label,ul_class_label,4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c6e92-4fe5-46e6-bb15-80180b9de718",
   "metadata": {},
   "source": [
    "# Last Stage Learn all and forget all by Bounday Expanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726e2d6-a3b6-4bfd-b0c6-349772f6f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the last stage learn all and forget all together \n",
    "All_Learn_Forget_No_memory_No_forget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f2986-c2ff-41c9-a788-a51a34ed245a",
   "metadata": {},
   "source": [
    "# Incremental learning in 5 tasks with no memory bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e06939-1355-49a7-9924-682649a99472",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Incremental learning in 5 tasks and no memory bank\n",
    "Naive_CL()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe8808-3ad1-40e0-9290-a015b6a3f766",
   "metadata": {},
   "source": [
    "# Incremental learning in 5 tasks with Full memory bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22803a-2ed3-4a2b-810c-e69f52445c97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Full_Memory_CL()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac835139-427d-4092-8047-cbd72b23eedc",
   "metadata": {},
   "source": [
    "# Learn first then Forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ad0c4-f335-4e26-9e55-b74b07cb1843",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Learn_All()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb91b6-6cbe-467f-bbf8-a5d28e7e9937",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Learn_CL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62332001-9745-4acd-af6f-3607cf5dee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forget_UL_Boundary_Expansion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7920d493-875d-439c-acbe-6e6bc8d5fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget train list length 8232 test list length 2494 cl class label length 65 ul class label length 53\n",
      "train loader length 258 test loader length 78 cl class label [15, 17, 60, 50, 32, 24, 63, 36, 31, 40, 52, 4, 25, 48, 54, 35, 62, 13, 42, 37, 49, 51, 45, 44, 14, 5, 46, 18, 57, 28, 11, 30, 61, 27, 22, 2, 29, 0, 19, 3, 59, 10, 12, 8, 1, 26, 23, 34, 58, 64, 56, 41, 47, 20, 53, 39, 9, 21, 16, 38, 33, 43, 6, 7, 55] ul class label [0, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 58, 61, 62, 63, 64]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7527986f3740406e913b715955eefe51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0,Batch 0 Loss: 100.18904876708984\n",
      "Epoch:0,Batch 10 Loss: 41.5723876953125\n",
      "Epoch:0,Batch 20 Loss: 30.053726196289062\n",
      "Epoch:0,Batch 30 Loss: 17.66861915588379\n",
      "Epoch:0,Batch 40 Loss: 21.125993728637695\n",
      "Epoch:0,Batch 50 Loss: 16.48838996887207\n",
      "Epoch:0,Batch 60 Loss: 16.23040008544922\n",
      "Epoch:0,Batch 70 Loss: 13.77851676940918\n",
      "Epoch:0,Batch 80 Loss: 12.48988151550293\n",
      "Epoch:0,Batch 90 Loss: 15.555898666381836\n",
      "Epoch:0,Batch 100 Loss: 12.741931915283203\n",
      "Epoch:0,Batch 110 Loss: 12.094322204589844\n",
      "Epoch:0,Batch 120 Loss: 13.497087478637695\n",
      "Epoch:0,Batch 130 Loss: 11.989791870117188\n",
      "Epoch:0,Batch 140 Loss: 12.87718677520752\n",
      "Epoch:0,Batch 150 Loss: 12.588712692260742\n",
      "Epoch:0,Batch 160 Loss: 12.647796630859375\n",
      "Epoch:0,Batch 170 Loss: 12.895086288452148\n",
      "Epoch:0,Batch 180 Loss: 11.810033798217773\n",
      "Epoch:0,Batch 190 Loss: 11.024385452270508\n",
      "Epoch:0,Batch 200 Loss: 11.824185371398926\n",
      "Epoch:0,Batch 210 Loss: 12.377079010009766\n",
      "Epoch:0,Batch 220 Loss: 11.775630950927734\n",
      "Epoch:0,Batch 230 Loss: 11.742414474487305\n",
      "Epoch:0,Batch 240 Loss: 11.965548515319824\n",
      "Epoch:0,Batch 250 Loss: 12.013715744018555\n",
      "Epoch 0 | Training Loss: 16.531014307524806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation starting ...: 100%|| 78/78 [01:13<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned target_acc and clipwise_output_acc\n",
      "Task 4 |  Current Epoch 0 | statistics {'cl_weighted_accuracy': 0.09087767451659721, 'ul_weighted_accuracy': 0.0, 'cl_accuracy': 0.1393643031784841, 'ul_accuracy': 0.0}\n",
      "Save Model Successfully /root/CLUL/run/Forget_UL_Random_label/task4best_epoch.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/root/miniconda3/envs/CLUL/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2399: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37571cea084949c386cf2821f0e4db33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1,Batch 0 Loss: 11.084427833557129\n",
      "Epoch:1,Batch 10 Loss: 11.884904861450195\n",
      "Epoch:1,Batch 20 Loss: 11.206670761108398\n",
      "Epoch:1,Batch 30 Loss: 11.606139183044434\n",
      "Epoch:1,Batch 40 Loss: 11.836101531982422\n",
      "Epoch:1,Batch 50 Loss: 11.80435848236084\n",
      "Epoch:1,Batch 60 Loss: 11.207924842834473\n",
      "Epoch:1,Batch 70 Loss: 10.596290588378906\n",
      "Epoch:1,Batch 80 Loss: 11.524690628051758\n",
      "Epoch:1,Batch 90 Loss: 10.76893138885498\n",
      "Epoch:1,Batch 100 Loss: 11.463611602783203\n",
      "Epoch:1,Batch 110 Loss: 10.96826171875\n",
      "Epoch:1,Batch 120 Loss: 11.645011901855469\n",
      "Epoch:1,Batch 130 Loss: 11.59510612487793\n",
      "Epoch:1,Batch 140 Loss: 10.997419357299805\n",
      "Epoch:1,Batch 150 Loss: 10.763168334960938\n",
      "Epoch:1,Batch 160 Loss: 11.835344314575195\n",
      "Epoch:1,Batch 170 Loss: 11.325338363647461\n",
      "Epoch:1,Batch 180 Loss: 11.078719139099121\n",
      "Epoch:1,Batch 190 Loss: 11.008458137512207\n",
      "Epoch:1,Batch 200 Loss: 11.467052459716797\n",
      "Epoch:1,Batch 210 Loss: 11.333417892456055\n",
      "Epoch:1,Batch 220 Loss: 11.161840438842773\n",
      "Epoch:1,Batch 230 Loss: 10.836978912353516\n",
      "Epoch:1,Batch 240 Loss: 10.866474151611328\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/transformers/feature_extraction_utils.py:183\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 183\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/transformers/feature_extraction_utils.py:160\u001b[0m, in \u001b[0;36mBatchFeature._get_is_as_tensor_fns.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    159\u001b[0m         value \u001b[38;5;241m=\u001b[39m as_tensor([np\u001b[38;5;241m.\u001b[39masarray(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m value], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mForget_UL_Random_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 175\u001b[0m, in \u001b[0;36mForget_UL_Random_label\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m train_list \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_list)[(pd\u001b[38;5;241m.\u001b[39mDataFrame(train_list)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(ytvos_category_dict))\u001b[38;5;241m.\u001b[39misin(ul_class_label)]\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforget train list length\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(train_list),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest list length\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(test_list),\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcl class label length\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(cl_class_label),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul class label length\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mlen\u001b[39m(ul_class_label))\n\u001b[0;32m--> 175\u001b[0m \u001b[43mclul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAll_Learn_then_Forget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcl_class_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43mul_class_label\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 569\u001b[0m, in \u001b[0;36mCLUL.train\u001b[0;34m(self, mode, train_list, test_list, cl_class_label, ul_class_label, cur_iter)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# print(\"=============This is input data==============\")\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# print(batch_data_dict['target'])\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 569\u001b[0m batch_output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwaveform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m batch_target_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mAudio_Encoder.forward\u001b[0;34m(self, audios)\u001b[0m\n\u001b[1;32m     24\u001b[0m input_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m audios:\n\u001b[0;32m---> 27\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_features\n\u001b[1;32m     28\u001b[0m     input_features\u001b[38;5;241m.\u001b[39mappend(feature)\n\u001b[1;32m     30\u001b[0m input_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(input_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/transformers/models/whisper/feature_extraction_whisper.py:282\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, device, return_token_timestamps, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m batched_speech \u001b[38;5;241m=\u001b[39m BatchFeature({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_speech})\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# convert into correct format for padding\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched_speech\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# zero-mean and unit-variance normalization\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/transformers/feature_extraction_sequence_utils.py:225\u001b[0m, in \u001b[0;36mSequenceFeatureExtractor.pad\u001b[0;34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[0m\n\u001b[1;32m    222\u001b[0m             value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    223\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchFeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/transformers/feature_extraction_utils.py:79\u001b[0m, in \u001b[0;36mBatchFeature.__init__\u001b[0;34m(self, data, tensor_type)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, tensor_type: Union[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m, TensorType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/transformers/feature_extraction_utils.py:189\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_values\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing values of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 189\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate padding \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "Forget_UL_Random_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6df90c27-fe70-4b07-92f6-d2af1dd2be8a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     cl_class_label \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cl_class_label_\n\u001b[1;32m     14\u001b[0m     ul_class_label \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ul_class_label_\n\u001b[0;32m---> 16\u001b[0m \u001b[43mLearn_CL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 82\u001b[0m, in \u001b[0;36mLearn_CL\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLearn_CL\u001b[39m():\n\u001b[0;32m---> 82\u001b[0m     clul \u001b[38;5;241m=\u001b[39m \u001b[43mCLUL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLearn_CL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     clul\u001b[38;5;241m.\u001b[39mchange_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/root/CLUL/run/All_Learn_then_Forget/task4best_epoch1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m     train_list,test_list \u001b[38;5;241m=\u001b[39m [],[]\n",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m, in \u001b[0;36mCLUL.__init__\u001b[0;34m(self, batch_size, lr, memory_size, forget_size, epoch, loss, total_class_num, mode, patience, n_worker, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m,memory_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      3\u001b[0m              forget_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,loss \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfocal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m              total_class_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65\u001b[39m,mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll_Learn_then_Forget\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m              patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,n_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      6\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      7\u001b[0m     feature_extractor \u001b[38;5;241m=\u001b[39m AutoFeatureExtractor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/CLUL/whisper\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     whisper_model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m Audio_Encoder(feature_extractor, whisper_model)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/whisper/__init__.py:150\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[1;32m    149\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 150\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/whisper/model.py:232\u001b[0m, in \u001b[0;36mWhisper.__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m=\u001b[39m dims\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m AudioEncoder(\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_mels,\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_layer,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mTextDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# use the last half among the decoder layers for time alignment by default;\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to use a specific set of heads, see `set_alignment_heads()` below.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m all_heads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_head, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool\n\u001b[1;32m    243\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/whisper/model.py:186\u001b[0m, in \u001b[0;36mTextDecoder.__init__\u001b[0;34m(self, n_vocab, n_ctx, n_state, n_head, n_layer)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(n_vocab, n_state)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_state))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks: Iterable[ResidualAttentionBlock] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 186\u001b[0m     [\n\u001b[1;32m    187\u001b[0m         ResidualAttentionBlock(n_state, n_head, cross_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)\n\u001b[1;32m    189\u001b[0m     ]\n\u001b[1;32m    190\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n\u001b[1;32m    193\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_ctx)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\u001b[38;5;241m.\u001b[39mtriu_(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/whisper/model.py:187\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(n_vocab, n_state)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_state))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks: Iterable[ResidualAttentionBlock] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    186\u001b[0m     [\n\u001b[0;32m--> 187\u001b[0m         \u001b[43mResidualAttentionBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)\n\u001b[1;32m    189\u001b[0m     ]\n\u001b[1;32m    190\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n\u001b[1;32m    193\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_ctx)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\u001b[38;5;241m.\u001b[39mtriu_(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/whisper/model.py:125\u001b[0m, in \u001b[0;36mResidualAttentionBlock.__init__\u001b[0;34m(self, n_state, n_head, cross_attention)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state) \u001b[38;5;28;01mif\u001b[39;00m cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m n_mlp \u001b[38;5;241m=\u001b[39m n_state \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mlp\u001b[49m\u001b[43m)\u001b[49m, nn\u001b[38;5;241m.\u001b[39mGELU(), Linear(n_mlp, n_state)\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_ln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniconda3/envs/CLUL/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#In the last stage learn all then forget all \n",
    "# All_Learn_then_Forget()\n",
    "# forget_all = [35, 3, 10, 46, 15, 48, 17, 18, 54, 57, 59, 60]\n",
    "# All_Forget()\n",
    "# clul = CLUL()\n",
    "# cl_class_label,ul_class_label = [],[]\n",
    "# train_list,test_list = [],[]\n",
    "# for idx in range(5):\n",
    "#     train_list_,test_list_ = clul.get_train_test_datalist(idx)\n",
    "#     cl_class_label_,ul_class_label_ =clul.get_cl_ul_class_label(idx)\n",
    "#     train_list += train_list_\n",
    "#     test_list += test_list_\n",
    "#     cl_class_label += cl_class_label_\n",
    "#     ul_class_label += ul_class_label_\n",
    "\n",
    "# Learn_CL()\n",
    "clul = CLUL()\n",
    "train_list,test_list = clul.get_train_test_datalist(0)\n",
    "train_loader,test_loader = get_train_test_dataloader(clul.batch_size, clul.n_worker, train_list, test_list)\n",
    "for tra in train_loader:\n",
    "    print(tra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab015a-9634-4fd6-b77b-ff4e8b93d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clul = CLUL()\n",
    "train_list,test_list = clul.get_train_test_datalist(0)\n",
    "train_loader,test_loader = get_train_test_dataloader(clul.batch_size, clul.n_worker, train_list, test_list)\n",
    "for tra in train_loader:\n",
    "    print(tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f418b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list1,test_list1 = clul.get_train_test_datalist(0)\n",
    "cl_class_label1,ul_class_label1 = clul.get_cl_ul_class_label(0)\n",
    "\n",
    "train_list2,test_list2 = clul.get_train_test_datalist(1)\n",
    "cl_class_label2,ul_class_label2 = clul.get_cl_ul_class_label(1)\n",
    "\n",
    "train_list = train_list1 + train_list2\n",
    "cl_class_label = cl_class_label1 + cl_class_label2\n",
    "ul_class_label = ul_class_label1 + ul_class_label2\n",
    "\n",
    "\n",
    "clul.change_model(r'C:\\Users\\Administrator\\Desktop\\CLUL-main\\run\\CLUL _no_foget_bank\\task1best_epoch2.pt')\n",
    "memory = clul.single_mutual_info_sampling(1,train_list,cl_class_label,ul_class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d49df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_label_dic\n",
    "train_list,test_list = get_datalist(0)\n",
    "train_df = pd.DataFrame(train_list)\n",
    "train_list,train_df\n",
    "for i in range(1000):\n",
    "    print(train_list[i]['video'] == train_df.iloc[i]['video'])\n",
    "train_df\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "from sklearn.metrics import f1_score,precision_recall_curve,precision_score,recall_score,accuracy_score,balanced_accuracy_score\n",
    "y_true = [0, 0, 0, 1, 1, 1,   2]\n",
    "y_pred = [0, 0, 0, 1, 1, 1,   4]\n",
    "class AverageMethod(str, Enum):\n",
    "    MICRO = 'micro'\n",
    "    WEIGHTED = 'weighted'\n",
    "    MACRO = 'macro'\n",
    "def evaluate(y_true,y_pred,average:AverageMethod):\n",
    "    statistics = {}\n",
    "    statistics['f1_score'] = f1_score(y_true,y_pred,average=average.value)\n",
    "    statistics['precision'] = precision_score(y_true,y_pred,average=average.value)\n",
    "    statistics['recall'] = recall_score(y_true,y_pred,average=average.value)\n",
    "    statistics['accuracy'] = accuracy_score(y_true,y_pred)\n",
    "    return statistics\n",
    "evaluate(y_true,y_pred,AverageMethod.MACRO),balanced_accuracy_score(y_true,y_pred)\n",
    "\n",
    "\n",
    "# train_list,test_list = get_datalist(0)\n",
    "# train_loader ,test_loader = get_train_test_dataloader(16,0,train_list,test_list)\n",
    "\n",
    "# for train in train_loader:\n",
    "#     print(train)\n",
    "\n",
    "\n",
    "# def train_total():\n",
    "#     for task_id in range(5):\n",
    "#         train_list,test_list,cl_class_label,ul_class_label = clul.train_with_forget_without_forget_bank(task_id)\n",
    "#         if task_id == 0:\n",
    "#             clul.equal_class_sampling(train_list)\n",
    "#         else:\n",
    "#             clul.single_mutual_info_sampling(train_list,cl_class_label,ul_class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6d381-9ff9-429d-83ee-197ff6f3cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "a = [1,2,3,4,5]\n",
    "for i in tqdm(a):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e9cbc-c3f4-4b0d-97c0-ccc2b90f32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button\n",
    "\n",
    "button = Button(description=\"Click me!\")\n",
    "button\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b6505-c203-426f-af70-25588633415d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7881c-d3c1-4582-a1de-c0bb8d5dd4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "CLUL",
   "language": "python",
   "name": "clul"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
