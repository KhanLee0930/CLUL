{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf742fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If U are using SageMaker Prepare for the dataset\n",
    "!pip install awscli\n",
    "!aws s3 cp s3://handata/ref_youtube_audio/ ref_youtube_audio/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install -U openai-whisper\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e857f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import whisper\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from categories import ytvos_category_dict\n",
    "import numpy as np\n",
    "from util import read_json,read_wav\n",
    "\n",
    "class Audio_Encoder(nn.Module):\n",
    "    def __init__(self, feature_extractor, model, num_class=66,dropout_prob=0.2,pool_num = 100,bias = True):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.encoder = model.encoder\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "          param.requires_grad = False\n",
    "        self.projector = nn.Linear(in_features=768, out_features=256, bias=True)\n",
    "        self.classifier = nn.Linear(256, num_class)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=(pool_num,1), stride=(pool_num,1))\n",
    "        # self.norm_layer = nn.LayerNorm(256, eps=1e-5, bias=True)\n",
    "        self.batchnorm = nn.BatchNorm1d(2048, affine=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(1500//pool_num * 256, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 256)\n",
    "        self.fc3 = nn.Linear(256, 65)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, audios):\n",
    "        input_features = []\n",
    "        for audio in audios:\n",
    "\n",
    "            feature = self.feature_extractor(audio.cpu(),sampling_rate=16000,return_tensors=\"pt\").input_features\n",
    "            input_features.append(feature)\n",
    "\n",
    "\n",
    "        input_features = torch.cat(input_features, dim=0).to(self.device)\n",
    "        hidden_states = self.encoder(input_features)\n",
    "        # hidden_states = self.projector(hidden_states)\n",
    "        # pooled_output = hidden_states.mean(dim=1)\n",
    "        # logits = self.classifier(pooled_output)\n",
    "\n",
    "        x = self.avg_pool(hidden_states)\n",
    "\n",
    "        x = self.projector(x)\n",
    "        # x = self.positionencoding(x)\n",
    "        feature = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.dropout(feature)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        # x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        output_dict = {\n",
    "            'clipwise_output': x,\n",
    "            'feature': feature,\n",
    "            'embedding': hidden_states}\n",
    "\n",
    "        return output_dict\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "whisper_model = whisper.load_model(\"small\")\n",
    "model = Audio_Encoder(feature_extractor, whisper_model)\n",
    "def print_trainable_para(model):\n",
    "    total_params = []\n",
    "    trainable_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params.append((name, param.numel()))\n",
    "        total_params.append((name,param.numel()))\n",
    "        \n",
    "    total_params_ = 0\n",
    "    trainable_params_ = 0\n",
    "    for name, num_params in total_params:\n",
    "        print(f\"Parameter name: {name}, Number of parameters: {num_params}\")\n",
    "        total_params_ += num_params\n",
    "    for name, num_params in trainable_params:\n",
    "        print(f\"Parameter name: {name}, Number of parameters: {num_params}\")\n",
    "        trainable_params_ += num_params\n",
    "        \n",
    "    print(f\"Total parameters: {total_params_}\")\n",
    "    print(f\"Trainable parameters: {trainable_params_}\")\n",
    "print_trainable_para(model)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d3e9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datalist(cur_iter):\n",
    "        task_id = cur_iter\n",
    "        task_train_metas = []\n",
    "        task_test_metas = []\n",
    "\n",
    "\n",
    "        metas = read_json('task_split_1/metas.json')['metas']\n",
    "        tasks = read_json('task_split_1/task{}.json'.format(task_id))[str(task_id)]\n",
    "\n",
    "        for category,task_metas_dict in tasks.items():\n",
    "            train_ids = task_metas_dict['train']\n",
    "            test_ids = task_metas_dict['test']\n",
    "            for train_id in train_ids:\n",
    "                task_train_metas.append(metas[train_id])\n",
    "            for test_id in test_ids:\n",
    "                task_test_metas.append(metas[test_id])\n",
    "\n",
    "        return task_train_metas,task_test_metas\n",
    "\n",
    "\n",
    "\n",
    "train_list,test_list = get_datalist(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a734c62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ytvos_Dataset(Dataset):\n",
    "    def __init__(self, data_frame: pd.DataFrame, sr=44100, num_class=65):\n",
    "        self.data_frame = data_frame\n",
    "        self.sr = sr\n",
    "        self.num_class = num_class\n",
    "        self.data_root = '/home/user/SED_Adaptation_Classifier-main/data/ref_youtube_audio/audio'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        audio_name = self.data_frame.iloc[index][\"video\"]\n",
    "        audio_id = self.data_frame.iloc[index][\"audio\"]\n",
    "        audio_path = 'ref_youtube_audio/audio' + '/' + audio_name + '/' + audio_id + '.wav'\n",
    "        name = audio_name + self.data_frame.iloc[index][\"exp\"]\n",
    "\n",
    "        waveform = read_wav(file_key)\n",
    "#         waveform = whisper.load_audio(audio_path,sr = 16000)\n",
    "\n",
    "\n",
    "        tag = self.data_frame.iloc[index][\"category\"]\n",
    "        target = ytvos_category_dict[self.data_frame.iloc[index][\"category\"]]\n",
    "        target = np.eye(self.num_class)[target]\n",
    "        data_dict = {'audio_name': name, 'waveform': waveform, 'target': target, 'tag': tag}\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "def default_collate_fn(batch):\n",
    "    audio_name = [data['audio_name'] for data in batch]\n",
    "    waveform = [torch.from_numpy(data['waveform']) for data in batch]\n",
    "    target = [data['target'] for data in batch]\n",
    "\n",
    "    # waveform = torch.FloatTensor(waveform)\n",
    "    # waveform = pad_sequence(waveform, batch_first=True, padding_value=0)\n",
    "    target = torch.FloatTensor(target)\n",
    "\n",
    "    return {'audio_name': audio_name, 'waveform': waveform, 'target': target}\n",
    "\n",
    "def get_dataloader(data_frame, dataset, split, batch_size, num_class, num_workers=8):\n",
    "    assert dataset == \"ref_youtube_audio\"\n",
    "    dataset = ytvos_Dataset(data_frame=data_frame)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                      shuffle=True, drop_last=False,\n",
    "                      num_workers=num_workers, collate_fn=default_collate_fn)\n",
    "\n",
    "def get_train_test_dataloader(batch_size, n_worker, train_list, test_list):\n",
    "    train_loader = get_dataloader(pd.DataFrame(train_list), 'ref_youtube_audio', split='train', batch_size=batch_size, num_class=66,\n",
    "                                  num_workers=n_worker)\n",
    "    test_loader = get_dataloader(pd.DataFrame(test_list), 'ref_youtube_audio', split='test', batch_size=batch_size, num_class=66,\n",
    "                                 num_workers=n_worker)\n",
    "    return train_loader, test_loader\n",
    "train_loader, test_loader = get_train_test_dataloader(16,8,train_list,test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28552e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7466bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3d49df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2bb0f7-8fe9-443c-a62c-04a0d6fb0fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "CLUL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
